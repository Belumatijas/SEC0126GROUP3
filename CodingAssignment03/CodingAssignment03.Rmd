---
title: "Coding Assignment 3"
author: "Team 3"
date: "Due: 2021-12-11 23:59"
output:
  html_document:
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
library(readxl)
library(plotly)
library(car)
library(corrplot)
library(jtools)
library(gt)
library(gtsummary)
library(ggplot2)
library(dummy)
library(errors)

knitr::opts_chunk$set(echo = TRUE)


```

A Florida health insurance company wants to predict annual claims for individual clients. The company pulls a random sample of 50 customers. The owner wishes to charge an actuarially fair premium to ensure a normal rate of return. The owner collects all of their current customer’s health care expenses from the last year and compares them with what is known about each customer’s plan. 

The data on the 50 customers in the sample is as follows:

-	Charges: Total medical expenses for a particular insurance plan (in dollars)
-	Age: Age of the primary beneficiary
-	BMI: Primary beneficiary’s body mass index (kg/m2)
-	Female: Primary beneficiary’s birth sex (0 = Male, 1 = Female)
-	Children: Number of children covered by health insurance plan (includes other dependents as well)
-	Smoker: Indicator if primary beneficiary is a smoker (0 = non-smoker, 1 = smoker)
-	Cities: Dummy variables for each city with the default being Sanford

Answer the following questions using complete sentences and attach all output, plots, etc. within this report.


```{r dataset, include=FALSE}
insurance <- read.csv("~/GitHub/SEC0126GROUP3/Data/insurance_0126_Group3.csv")
summary(insurance)
```

## Question 1

Randomly select three observations from the sample and exclude from all modeling (i.e. n=47). Provide the summary statistics (min, max, std, mean, median) of the quantitative variables for the 47 observations.

```{r Q1}
set.seed(123456)
index <- sample(seq_len(nrow(insurance)), size = 3) 

training_set <- insurance[-index,] #everything but the three rows we dropped / 47 observations

testing_set <- insurance[index,] #the three rows we pulled put. 

summary(training_set[,c(1:3,5)]) # only looking at the quantitative variables and the 47 observations. 

sd(training_set$Charges, na.rm = TRUE)

sd(training_set$Age, na.rm = TRUE)

sd(training_set$BMI, na.rm = TRUE)

sd(training_set$Children, na.rm = TRUE)



summary(training_set)

summary(testing_set)

training_set %>% 
  tbl_summary(statistic = list(all_continuous() ~ c("{mean} ({sd})",
                                                    "{median} ({p25}, {p75})",
                                                    "{min}, {max}"),
                              all_categorical() ~ "{n} / {N} ({p}%)"),
              type = all_continuous() ~ "continuous2"
  )
```
Charges is skewed to the Right because the mean is greater than the median, which means there might be some outliers. 

Age has a approximately normal distribution because the mean and median are similar. 

BMI has a approximately normal distribution because the mean and median are similar.   

**Children is skewed to the Right because the mean is greater than the median, which means there might be some outliers. 

## Question 2

Provide the correlation between all quantitative variables

```{r Q2}
cor(training_set[c(1:3,5)])
corrplot((cor(training_set[,c(1:3,5)])))
```
Based on the data above, all the values are below .5 therefore indicating that they are not highly correlated. No evidence of multicollinearity. 

## Question 3

Run a regression that includes all independent variables in the data table. Does the model above violate any of the Gauss-Markov assumptions? If so, what are they and what is the solution for correcting?
```{r Q3}
model_1 <- lm(Charges ~. , data = training_set)
summary(model_1)
par(mfrow=c(2,2))
plot(model_1)

```
The 3rd Gauss Markov assumption is violated in the top left plot, Residuals vs Fitted, because it displays a non linear relationship as the line is not horizontal. 

The 6th Gauss Markov assumption is violated in the top right plot, Normal Q-Q, because the values are not normally distributed as there are values that leave the linear distribution.

The bottom left plot, Scale-Location, is not perfectly horizontal and the data points fan out, so it violates the 4th Gauss Markov assumption.

In the bottom right plot,Residuals vs Leverage, no Gauss Markov violations are present as there are no outliers that go beyond Cook's Distance.

The intercept along with the coefficients of WinterSprings, WinterPark, and Oviedo which are also dummy variables, are the only ones that have a negative sign.

The only significant variables are Age, BMI, and Smoker as their P-values are less than .05 in a 95% confidence interval.

```{r q3.2}
scatterplotMatrix(training_set[-c(4,6:11)])

```
From here you can see some non-linear relationships and non-normally distributed variables. Transformations are needed in order to account for the nonlinear relationships.


Age and Charges appear to not have a linear relationship, we will transform this by a log function. Taking logs will bring outliers closer to the other charges.
BMI & Charges looks to have a heteroskedasticity relationship, we will fix with a log function. 
Children& Charges appear to not have a linear relationship, we will transform this by a log function. Taking logs will bring outliers closer to the other charges

## Question 4

Implement the solutions from question 3, such as data transformation, along with any other changes you wish. Use the sample data and run a new regression. How have the fit measures changed? How have the signs and significance of the coefficients changed?

```{r Q4 1 transformation}
training_set$LnAge <- log(training_set$Age)
                                 
hist(training_set$Age) #before
hist(training_set$LnAge) #after
scatterplotMatrix(training_set[c(1,2,10)])


model_transformation1 <- lm(Charges ~., data = training_set[,c(10,1,4,6:9)])
summary(model_transformation1)


#or

tbl_regression(model_transformation1,
               estimate_fun =  ~style_sigfig(.x, digits = 4)) %>% as_gt() %>%
  gt::tab_source_note(gt::md(paste0("Adjusted R-Squared: ",round(summary(model_transformation1)$adj.r.squared* 100,digits = 2),"%")))


par(mfrow=c(2,2))
plot(model_transformation1)
```

The following changes have occurred after the data transformation:

The intercept along with the coefficients of WinterSprings, WinterPark, and Oviedo are still negative coefficients.In addition, Female is now negative as a result of the transformation.


The variables that are now significant are LnAge and WinterSprings as their p-value is less than .05 in a 95% confidence interval. Smoker is still significant as it was in the original regression.




```{r Q4 2 transformation}

training_set$LnCharges <- log(training_set$Charges)
                                 
hist(training_set$Charges) #before
hist(training_set$LnCharges) #after
scatterplotMatrix(training_set[c(1,2,11)])


model_transformation2 <- lm(Charges ~., data = training_set[,c(11,1,4,6:9)])
summary(model_transformation2)
# or

tbl_regression(model_transformation2,
               estimate_fun =  ~style_sigfig(.x, digits = 4)) %>% as_gt() %>%
  gt::tab_source_note(gt::md(paste0("Adjusted R-Squared: ",round(summary(model_transformation2)$adj.r.squared* 100,digits = 2),"%")))

par(mfrow=c(2,2))
plot(model_transformation2)
```

The following changes have occurred after the data transformation:

The intercept along with Female, WinterSprings, WinterPark, and Oviedo are negative coefficients.
The intercept, LnCharges, and Smoker test significant.

The data is more distributed and the Mean lies closer to the middle.


```{r Q4 3 }

training_set$BMIsqd <- training_set$BMI ^2


hist(training_set$BMI)#Before
hist(training_set$BMI^2)#After

scatterplotMatrix(training_set[c(1,3,12)])


model_transformation3 <- lm(Charges ~., data = training_set[,c(12,1,3,4,6:9)])
summary(model_transformation3)
# or

tbl_regression(model_transformation3,
               estimate_fun =  ~style_sigfig(.x, digits = 4)) %>% as_gt() %>%
  gt::tab_source_note(gt::md(paste0("Adjusted R-Squared: ",round(summary(model_transformation3)$adj.r.squared* 100,digits = 2),"%")))

par(mfrow=c(2,2))
plot(model_transformation3)


```










```{r}


 
testing_set$LnAge <- log(testing_set$Age)
testing_set$LnCharges <- log(testing_set$Charges)



testing_set$model_1_pred <- predict(model_transformation1,newdata = testing_set) %>% exp()

testing_set$model_2_pred <- predict(model_transformation2,newdata = testing_set) %>% exp()


testing_set$error_1 <- testing_set$model_1_pred - testing_set$Charges

testing_set$error_2 <- testing_set$model_2_pred - testing_set$Charges



```


## Question 5


Use the 3 withheld observations and calculate the performance measures for your best two models. Which is the better model? (remember that "better" depends on whether your outlook is short or long run)

test$error <- test$Prediction - test$Charges

```{r Q5}

# Model 1
mean(testing_set$error_1)
##

# Model 2
mean(testing_set$error_2)
## 


mae <- function(error_vector){
error_vector %>%
abs() %>%
mean()
}



# Model 1
mae(testing_set$error_1)
## [1] 

# Model 2
mae(testing_set$error_2)
## [1] 

 
rmse <- function(error_vector){
error_vectorˆ2 %>%
mean() %>%
sqrt()}



# Model 1
rmse(testing_set$error_1)

##  

# Model 2
rmse(testing_set$error_2)
## 

 
mape <- function(error_vector, actual_vector){
(error_vector/actual_vector) %>%
abs() %>%
mean()
}



# Model 1
mape(test$error_1, test$saleprice)
## 

# Model 2
mape(testing_set$error_2, test$saleprice)
## 
```
Looking at these three models, the initial model was the worst performing.. Looking at the other two, the logarithmic relationship has lower bias, MAE, and MAPE. Model 2 has a lower RMSE meaning that there were not large prediction errors. Picking which model would depend on your time preference. If you are looking at the short-run, then Model 2. Model 1 if you are looking at the long-run. Needs edits.


## Question 6

Provide interpretations of the coefficients, do the signs make sense? Perform marginal change analysis (thing 2) on the independent variables.

```{r Q6}

summary(model_transformation1)
```

## Question 7

An eager insurance representative comes back with five potential clients. Using the better of the two models selected above, provide the prediction intervals for the five potential clients using the information provided by the insurance rep.

| Customer | Age | BMI | Female | Children | Smoker | City           |
| -------- | --- | --- | ------ | -------- | ------ | -------------- | 
| 1        | 60  | 22  | 1      | 0        | 0      | Oviedo         |
| 2        | 40  | 30  | 0      | 1        | 0      | Sanford        |
| 3        | 25  | 25  | 0      | 0        | 1      | Winter Park    |
| 4        | 33  | 35  | 1      | 2        | 0      | Winter Springs |
| 5        | 45  | 27  | 1      | 3        | 0      | Oviedo         |


```{r Q7}
####### need to decide whether the training model or the testing model is best. Then adjust the the formulas to reflect...I think ######
model_1 <- lm(Charges ~ Age + BMI + Children + Female + Smoker + WinterSprings + WinterPark + Oviedo, data = training_set)

Customer1 <- data.frame(Age = 60,
BMI = 22,
Female = 1,
Children = 0,
Smoker = 0,
City = insurance$Oviedo)
predict(model_1, newdata = training_set,interval = "prediction")
predict(model_1, newdata = training_set,interval = "confidence")

Customer2 <- data.frame(Age = 40,
BMI = 30,
Female = 0,
Children = 1,
Smoker = 0,
City = dummy(default)
predict(model_1, newdata = training_set,interval = "prediction")
predict(model_1, newdata = training_set,interval = "confidence")

Customer3 <- data.frame(Age = 25,
BMI = 25,
Female = 0,
Children = 0,
Smoker = 1,
City = insurance$WinterPark)

predict(model_1, newdata = training_set, interval = "confidence", level = .95)


Customer4 <- data.frame(Age = 33,
BMI = 35,
Female = 1,
Children = 2,
Smoker = 0,
City = insurance$WinterSprings)
predict(model_1, newdata = training_set,interval = "prediction")
predict(model_1, newdata = training_set,interval = "confidence")

Customer5 <- data.frame(Age = 45,
BMI = 27,
Female = 1,
Children = 3,
Smoker = 0,
City = insurance$Oviedo)
predict(model_1, newdata = training_set,interval = "prediction")
predict(model_1, newdata = training_set,interval = "confidence")
```

## Question 8

The owner notices that some of the predictions are wider than others, explain why.
 
"Because prediction intervals are concerned with the individual observations in a population as well as the parameter estimates, prediction intervals will necessarily be wider than a confidence interval calculated for the same data set. For the same reason, prediction intervals are also more susceptible to the assumption of normality than are confidence intervals."

https://www.propharmagroup.com/blog/understanding-statistical-intervals-part-2-prediction-intervals/

## Question 9 

Are there any prediction problems that occur with the five potential clients? If so, explain.


