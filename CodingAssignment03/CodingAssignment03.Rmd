---
title: "Coding Assignment 3"
author: "Team 3"
date: "Due: 2021-12-11 23:59"
output:
  html_document:
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
library(readxl)
library(plotly)
library(car)
library(corrplot)
library(jtools)
library(gt)
library(gtsummary)
library(ggplot2)
library(dummy)
library(errors)

knitr::opts_chunk$set(echo = TRUE)


```

A Florida health insurance company wants to predict annual claims for individual clients. The company pulls a random sample of 50 customers. The owner wishes to charge an actuarially fair premium to ensure a normal rate of return. The owner collects all of their current customer’s health care expenses from the last year and compares them with what is known about each customer’s plan. 

The data on the 50 customers in the sample is as follows:

-	Charges: Total medical expenses for a particular insurance plan (in dollars)
-	Age: Age of the primary beneficiary
-	BMI: Primary beneficiary’s body mass index (kg/m2)
-	Female: Primary beneficiary’s birth sex (0 = Male, 1 = Female)
-	Children: Number of children covered by health insurance plan (includes other dependents as well)
-	Smoker: Indicator if primary beneficiary is a smoker (0 = non-smoker, 1 = smoker)
-	Cities: Dummy variables for each city with the default being Sanford

Answer the following questions using complete sentences and attach all output, plots, etc. within this report.


```{r dataset, include=FALSE}
insurance <- read.csv("~/GitHub/SEC0126GROUP3/Data/insurance_0126_Group3.csv")
summary(insurance)
```

## Question 1

Randomly select three observations from the sample and exclude from all modeling (i.e. n=47). Provide the summary statistics (min, max, std, mean, median) of the quantitative variables for the 47 observations.

```{r Q1}
set.seed(123456)
index <- sample(seq_len(nrow(insurance)), size = 3) 

training_set <- insurance[-index,] #everything but the three rows we dropped / 47 observations

testing_set <- insurance[index,] #the three rows we pulled put. 

summary(training_set[,c(1:3,5)]) # only looking at the quantitative variables and the 47 observations. 

sd(training_set$Charges, na.rm = TRUE)

sd(training_set$Age, na.rm = TRUE)

sd(training_set$BMI, na.rm = TRUE)

sd(training_set$Children, na.rm = TRUE)



summary(training_set)

summary(testing_set)

training_set %>% 
  tbl_summary(statistic = list(all_continuous() ~ c("{mean} ({sd})",
                                                    "{median} ({p25}, {p75})",
                                                    "{min}, {max}"),
                              all_categorical() ~ "{n} / {N} ({p}%)"),
              type = all_continuous() ~ "continuous2"
  )
```
Charges is skewed to the Right because the mean is greater than the median, which means there might be some outliers. 

Age has a approximately normal distribution because the mean and median are similar. 

BMI has a approximately normal distribution because the mean and median are similar.   

**Children is skewed to the Right because the mean is greater than the median, which means there might be some outliers. 

## Question 2

Provide the correlation between all quantitative variables

```{r Q2}
cor(training_set[c(1:3,5)])
corrplot((cor(training_set[,c(1:3,5)])))
```
Based on the data above, all the values are below .5 therefore indicating that they are not highly correlated. No evidence of multicollinearity. 
BMI and Charges have a .456 correlation coefficient which indicates a moderate level of correlation. We will conduct further investigation on this relationship below. 

## Question 3

Run a regression that includes all independent variables in the data table. Does the model above violate any of the Gauss-Markov assumptions? If so, what are they and what is the solution for correcting?
```{r Q3}
model_1 <- lm(Charges ~. , data = training_set)
summary(model_1)
par(mfrow=c(2,2))
plot(model_1)

```
The 3rd Gauss Markov assumption is violated in the top left plot, Residuals vs Fitted, because it displays a non linear relationship as the line is not horizontal. 

The 6th Gauss Markov assumption is violated in the top right plot, Normal Q-Q, because the values are not normally distributed as there are values that leave the linear distribution.

The bottom left plot, Scale-Location, is not perfectly horizontal and the data points fan out, so it violates the 4th Gauss Markov assumption.

In the bottom right plot,Residuals vs Leverage, no Gauss Markov violations are present as there are no outliers that go beyond Cook's Distance.

The intercept along with the coefficients of WinterSprings, WinterPark, and Oviedo which are also dummy variables, are the only ones that have a negative sign.

The only significant variables are Age, BMI, and Smoker as their P-values are less than .05 in a 95% confidence interval.

```{r q3.2}
scatterplotMatrix(training_set[-c(4,6:11)])  #scatter plot of just the quantitative variables 

```
From here you can see some non-linear relationships and non-normally distributed variables. Transformations are needed in order to account for the nonlinear relationships.


Age and Charges appear to not have a linear relationship, we will transform this by a log function. Taking logs will bring outliers closer to the other charges.
BMI & Charges looks to have a heteroskedasticity relationship, we will fix with a log function. 
Children& Charges appear to not have a linear relationship, we will transform this by a log function. Taking logs will bring outliers closer to the other charges

## Question 4

Implement the solutions from question 3, such as data transformation, along with any other changes you wish. Use the sample data and run a new regression. How have the fit measures changed? How have the signs and significance of the coefficients changed?

```{r Q4 1 transformation}
training_set$LnCharges <- log(training_set$Charges)

training_set$LnAge <- log(training_set$Age)
                                 
hist(training_set$Age) #before
hist(training_set$LnAge) #after
scatterplotMatrix(training_set[c(2,11)]) #scatter plot of LnAge & Age 


model_transformation1 <- lm(LnAge ~., data = training_set[,c(11,3:9)])
summary(model_transformation1)


#or

tbl_regression(model_transformation1,
               estimate_fun =  ~style_sigfig(.x, digits = 4)) %>% as_gt() %>%
  gt::tab_source_note(gt::md(paste0("Adjusted R-Squared: ",round(summary(model_transformation1)$adj.r.squared* 100,digits = 2),"%")))


par(mfrow=c(2,2))
plot(model_transformation1)
```

The following changes have occurred after the data transformation:

The intercept along with the coefficients of WinterSprings, WinterPark, and Oviedo are still negative coefficients.In addition, Female is now negative as a result of the transformation.


The variables that are now significant are LnAge and WinterSprings as their p-value is less than .05 in a 95% confidence interval. Smoker is still significant as it was in the original regression.




```{r Q4 2 transformation}

training_set$LnCharges <- log(training_set$Charges)
                                 
hist(training_set$Charges) #before
hist(training_set$LnCharges) #after
scatterplotMatrix(training_set[c(1,10)])


model_transformation2 <- lm(LnCharges ~., data = training_set[,c(10,2:9)])
summary(model_transformation2)
# or

tbl_regression(model_transformation2,
               estimate_fun =  ~style_sigfig(.x, digits = 4)) %>% as_gt() %>%
  gt::tab_source_note(gt::md(paste0("Adjusted R-Squared: ",round(summary(model_transformation2)$adj.r.squared* 100,digits = 2),"%")))

par(mfrow=c(2,2))
plot(model_transformation2)
```

The following changes have occurred after the data transformation:

The intercept along with Female, WinterSprings, WinterPark, and Oviedo are negative coefficients.
The intercept, LnCharges, and Smoker test significant.

The data is more distributed and the Mean lies closer to the middle.


```{r Q4 3 }
training_set$LnCharges <- log(training_set$Charges)
training_set$BMIsqd <- training_set$BMI ^2


hist(training_set$BMI)#Before
hist(training_set$BMI^2)#After

scatterplotMatrix(training_set[c(3,12)])


model_transformation3 <- lm(LnCharges ~., data = training_set[,c(10,12,2,4:9)])
summary(model_transformation3)
# or

tbl_regression(model_transformation3,
               estimate_fun =  ~style_sigfig(.x, digits = 4)) %>% as_gt() %>%
  gt::tab_source_note(gt::md(paste0("Adjusted R-Squared: ",round(summary(model_transformation3)$adj.r.squared* 100,digits = 2),"%")))

par(mfrow=c(2,2))
plot(model_transformation3)


```


After the data transformation, the coefficients of WinterSprings, WinterPark, and Oviedo are negative. The only significant independent variables are BMIsqd, Age, and Smoker.

The histogram also reflects a more distributed set of data and the Mean lies closer to the center.


## Question 5


Use the 3 withheld observations and calculate the performance measures for your best two models. Which is the better model? (remember that "better" depends on whether your outlook is short or long run)

test$error <- test$Prediction - test$Charges
### Errors
```{r q5}


 
testing_set$LnAge <- log(testing_set$Age)
testing_set$LnCharges <- log(testing_set$Charges)



testing_set$model_1_pred <- predict(model_transformation1,newdata = testing_set) %>% exp()

testing_set$model_2_pred <- predict(model_transformation2,newdata = testing_set) %>% exp()


testing_set$error_1 <- testing_set$model_1_pred - testing_set$Charges

testing_set$error_2 <- testing_set$model_2_pred - testing_set$Charges



```
### Bias
```{r Q5 1}

# Model 1
mean(testing_set$error_1)
##[1] -20696.6

# Model 2
mean(testing_set$error_2)
## [1] 1630.76

```

### MAE
```{r Q5 2}
mae <- function(error_vector){
error_vector %>%
abs() %>%
mean()
}

# Model 1
mae(testing_set$error_1)
## [1] 20696.6

# Model 2
mae(testing_set$error_2)
## [1] 2851.449

 
```

### RMSE

```{r Q5 3}

rmse <- function(error_vector){
   error_vector^2 %>% 
  mean() %>% 
  sqrt()

}

# Model 1
rmse(testing_set$error_1)
##  [1] 26012.14

# Model 2
rmse(testing_set$error_2)
## [1] 2997.694

```

### MAPE

```{r Q5 4}
mape <- function(error_vector, actual_vector){
  (error_vector/actual_vector) %>% 
    abs() %>% 
    mean()
}

# Model 1
mape(testing_set$error_1, testing_set$Charges)
## [1] 0.9967447

# Model 2
mape(testing_set$error_2, testing_set$Charges)
## [1] 0.1859913
```
Looking at these two models, model one was the worst performing. Looking at Model two, the logarithmic relationship has positive bias ( which mean it over predicts), lower MAE, and lower MAPE. A MAPE less than 25%, which Model 2 has, means that a prediction has low, but acceptable accuracy. Model 2 also has a lower RMSE meaning that there were not large prediction errors. If you were to pick between one model or the other, it would depend on your time preference. If you are looking to choose in the short-run, then Model 2. If the long-run, we would still advise to use Model 2 because Model one has a very high MAPE. 


## Question 6

Provide interpretations of the coefficients, do the signs make sense? Perform marginal change analysis (thing 2) on the independent variables.

```{r Q6}

summary(model_transformation2)
```

Since Age, BMI, and Smoker all tested significant at 95% level, we will analyze those Coefficients.  
If a person's Age increases by 1, their charges would increase by $0.02 give or take $0.01. 
If a person's BMI increases by 1, their charges would increase by $0.04 give or take $0.03. 
If a person is a Smoker, their charges would increase by $1.64 give or take $0.35. 

## Question 7

An eager insurance representative comes back with five potential clients. Using the better of the two models selected above, provide the prediction intervals for the five potential clients using the information provided by the insurance rep.

| Customer | Age | BMI | Female | Children | Smoker | City           |
| -------- | --- | --- | ------ | -------- | ------ | -------------- | 
| 1        | 60  | 22  | 1      | 0        | 0      | Oviedo         |
| 2        | 40  | 30  | 0      | 1        | 0      | Sanford        |
| 3        | 25  | 25  | 0      | 0        | 1      | Winter Park    |
| 4        | 33  | 35  | 1      | 2        | 0      | Winter Springs |
| 5        | 45  | 27  | 1      | 3        | 0      | Oviedo         |


```{r Q7}

model_transformation2 <- lm(LnCharges ~., data = training_set[,c(10,2:6)])
summary(model_transformation2)

Customer1 <- data.frame(Age = 60,
BMI = 22,
Female = 1,
Children = 0,
Smoker = 0,
City = 1)
predict(model_transformation2, newdata = Customer1,interval = "prediction")
predict(model_transformation2, newdata = Customer1,interval = "confidence")


Customer2 <- data.frame(Age = 40,
BMI = 30,
Female = 0,
Children = 1,
Smoker = 0,
City = 0)
predict(model_transformation2, newdata = Customer2,interval = "prediction")
predict(model_transformation2, newdata = Customer2,interval = "confidence")

Customer3 <- data.frame(Age = 25,
BMI = 25,
Female = 0,
Children = 0,
Smoker = 1,
City = 1)
predict(model_transformation2, newdata = Customer3,interval = "prediction")
predict(model_transformation2, newdata = Customer3,interval = "confidence")


Customer4 <- data.frame(Age = 33,
BMI = 35,
Female = 1,
Children = 2,
Smoker = 0,
City = 1)
predict(model_transformation2, newdata = Customer4,interval = "prediction")
predict(model_transformation2, newdata = Customer4,interval = "confidence")

Customer5 <- data.frame(Age = 45,
BMI = 27,
Female = 1,
Children = 3,
Smoker = 0,
City = 1)
predict(model_transformation2, newdata = Customer5,interval = "prediction")
predict(model_transformation2, newdata = Customer5,interval = "confidence")
```

For Customer 1 we are 95% that they would be charged between $7.86 and $10.09
For person with similar characteristics of Customer 1 we are 95% that they would be charged between $8.55 and $9.39

For Customer 2 we are 95% that they would be charged between $7.65 and $9.77
For person with similar characteristics of Customer 2 we are 95% that they would be charged between $8.48 and $8.94

For Customer 3 we are 95% that they would be charged between $ 8.57 and$10.79
For person with similar characteristics of Customer 3 we are 95% that they would be charged between $9.29 and $10.07

For Customer 4 we are 95% that they would be charged between $8.01 and $10.21
For person with similar characteristics of Customer 4 we are 95% that they would be charged between $8.73 and $9.48

For Customer 5 we are 95% that they would be charged between $ 8.07 and $ 10.27
For person with similar characteristics of Customer 5 we are 95% that they would be charged between $8.79 and $9.55

## Question 8

The owner notices that some of the predictions are wider than others, explain why.

 
As stated by propharmagroup, "Because prediction intervals are concerned with the individual observations in a population as well as the parameter estimates, prediction intervals will necessarily be wider than a confidence interval calculated for the same data set."(propharmagroup.com)

Basically, this means that parameter estimates are added concerning the use of prediction intervals.

https://www.propharmagroup.com/blog/understanding-statistical-intervals-part-2-prediction-intervals/


## Question 9 

Are there any prediction problems that occur with the five potential clients? If so, explain.
```{r q9}
 summary(training_set[,c(1:3,5)]) # only looking at the quantitative variables and the 47 observations.
```


All data from each customer is within range for each quantitative variable. We do not see any evidence of extrapolation or outliers.
